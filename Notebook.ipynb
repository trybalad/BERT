{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": [],
   "authorship_tag": "ABX9TyPtc9Nec3oUAdv5ZPaxlSmi",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/trybalad/BERT/blob/master/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instalacja potrzebnych zależności"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download pl_core_news_lg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jeśli szkolimy z gpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jeśli wykorzystujemy cpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import potrzebnych plików i przygotowanie ustawień modelu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras_bert.data_generator import DataGenerator\n",
    "from keras_bert.model import create_model\n",
    "from keras_bert.tokenizer import Tokenizer\n",
    "from keras_bert.training import train_model\n",
    "\n",
    "\n",
    "max_len = 32\n",
    "embedding_dim = 512\n",
    "ff_dim = 512\n",
    "heads = 4\n",
    "encoder_num = 4\n",
    "\n",
    "checkpoint_file_path = \"./data/checkpoint_notebook.ckpt\"\n",
    "load_checkpoint = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utworzenie słownika na bazie dokumentu i zapisanie go w pliku"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "print(\"Preparing vocab.\")\n",
    "tokenizer.prepare_vocab(\"./data/corpus_clean.txt\", './data/vocab.txt')\n",
    "print(\"Vocab of size:\", tokenizer.vocab_size, \"created.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wczytanie słownika z pliku"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Reading vocab.\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.read_vocab('./data/counted_vocab.txt')\n",
    "tokenizer.change_to_reversible()\n",
    "print(\"Vocab of size:\", tokenizer.vocab_size, \"loaded.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utworzenie generatora danych treningowych"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Creating data generator\")\n",
    "data_generator = DataGenerator(\"./data/corpus_clean.txt\", max_len, tokenizer, batch_size=64, create_nsr_output=True)\n",
    "print(\"Data generator prepared.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stworzenie modelu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Preparing model\")\n",
    "sequence_encoder = create_model(tokenizer.vocab_size, max_len, embedding_dim, encoder_num, heads, ff_dim)\n",
    "print(\"Model created.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rozpoczęcie procesu treningu wstępnego"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Start training.\")\n",
    "train = train_model(sequence_encoder, max_len, tokenizer, data_generator, epochs=100,\n",
    "                    checkpoint_file_path=checkpoint_file_path, load_checkpoint=load_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing model by metric of masked/replaced words correctly predicted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras_bert.model import create_model\n",
    "from keras_bert.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "from keras_bert.prepare_data import create_tokens, create_masks, create_ids, create_segments, translate_ids, \\\n",
    "    create_pretrain_data\n",
    "from keras_bert.training import prepare_pretrain_model_from_checkpoint\n",
    "import codecs\n",
    "\n",
    "print(\"Test on validation data to compare with google-Keras metric.\")\n",
    "\n",
    "print(\"Reading vocab.\")\n",
    "validation_tokenizer = Tokenizer()\n",
    "validation_tokenizer.read_vocab('./data/counted_vocab.txt')\n",
    "validation_tokenizer.change_to_reversible()\n",
    "print(\"Vocab of size:\", validation_tokenizer.vocab_size, \"loaded.\")\n",
    "\n",
    "max_len = 32\n",
    "embedding_dim = 512\n",
    "ff_dim = 512\n",
    "heads = 4\n",
    "encoder_num = 4\n",
    "old_checkpoint = \"./data/checkpoint_test11.ckpt\"\n",
    "text_file = \"./data/corpus_clean.txt\"\n",
    "\n",
    "print(\"Preparing model.\")\n",
    "sequence_encoder = create_model(validation_tokenizer.vocab_size, max_len, embedding_dim, encoder_num, heads, ff_dim)\n",
    "model = prepare_pretrain_model_from_checkpoint(sequence_encoder, validation_tokenizer,  load_checkpoint=True, old_checkpoint=old_checkpoint)\n",
    "print(\"Model created.\")\n",
    "\n",
    "print(\"Starting test.\")\n",
    "file = codecs.open(text_file, 'r', 'utf-8')\n",
    "\n",
    "message = file.readline()\n",
    "count = 0\n",
    "sum_points = 0\n",
    "sum_correct = 0\n",
    "while message:\n",
    "    count += 1\n",
    "    tokens = create_tokens([message], validation_tokenizer, max_len)\n",
    "    mlm_tokens = create_pretrain_data(tokens, validation_tokenizer)\n",
    "    ids = create_ids(mlm_tokens, max_len, validation_tokenizer)\n",
    "    mask = create_masks(mlm_tokens, max_len)\n",
    "    segments = create_segments(mlm_tokens, max_len)\n",
    "    \n",
    "    result = model.predict(x = [np.array(ids), np.array(segments), np.array(mask)])\n",
    "    prediction = translate_ids(result, validation_tokenizer)\n",
    "    \n",
    "    good = 0\n",
    "    all_points = 0\n",
    "    for i in range(0, len(tokens)):\n",
    "        for j in range(0, len(tokens[i])):\n",
    "            if tokens[i][j] != mlm_tokens[i][j]:\n",
    "                all_points += 1\n",
    "                if tokens[i][j] == prediction[j]:\n",
    "                    good += 1\n",
    "                \n",
    "        sum_correct += good\n",
    "        sum_points += all_points\n",
    "        \n",
    "        if sum_points != 0 and count%100 == 0:\n",
    "            print(count,\"\\t\",(sum_correct/sum_points))\n",
    "        message = file.readline()\n",
    "print(sum_correct/sum_points)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-Tune process -- AR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras_bert.tuning.tuning_data_generator import TuningDataGenerator\n",
    "from keras_bert.model import create_model\n",
    "from keras_bert.tokenizer import Tokenizer\n",
    "from keras_bert.tuning.fine_tuning import fine_tune\n",
    "\n",
    "max_len = 32\n",
    "embedding_dim = 512\n",
    "ff_dim = 512\n",
    "heads = 4\n",
    "encoder_num = 4\n",
    "\n",
    "epochs = 10\n",
    "learn_type = \"ar\"\n",
    "old_checkpoint = \"./data/checkpoint_test10.ckpt\"\n",
    "new_checkpoint = \"./data/ar/checkpoint_ar.ckpt\"\n",
    "text_file = \"./data/ar/train.tsv\"\n",
    "\n",
    "print(\"Reading vocab.\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.read_vocab('./data/counted_vocab.txt')\n",
    "tokenizer.change_to_reversible()\n",
    "print(\"Vocab of size:\", tokenizer.vocab_size, \"loaded.\")\n",
    "\n",
    "data_generator = TuningDataGenerator(text_file, max_len, tokenizer, batch_size=64, tuning_type=learn_type)\n",
    "print(\"Data generator prepared.\")\n",
    "\n",
    "sequence_encoder = create_model(tokenizer.vocab_size, max_len, embedding_dim, heads, encoder_num, ff_dim)\n",
    "print(\"Model created.\")\n",
    "\n",
    "# Start training.\n",
    "train = fine_tune(sequence_encoder, tokenizer, max_len, data_generator, epochs=epochs,\n",
    "                    checkpoint_file_path=new_checkpoint, load_checkpoint=True, old_checkpoint=old_checkpoint, learn_type=learn_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-Tune process -- CBD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras_bert.tuning.tuning_data_generator import TuningDataGenerator\n",
    "from keras_bert.model import create_model\n",
    "from keras_bert.tokenizer import Tokenizer\n",
    "from keras_bert.tuning.fine_tuning import fine_tune\n",
    "\n",
    "max_len = 32\n",
    "embedding_dim = 512\n",
    "ff_dim = 512\n",
    "heads = 4\n",
    "encoder_num = 4\n",
    "\n",
    "epochs = 10\n",
    "learn_type = \"cbd\"\n",
    "old_checkpoint = \"./data/checkpoint_test10.ckpt\"\n",
    "new_checkpoint = \"./data/klej_cbd/checkpoint_cbd.ckpt\"\n",
    "text_file = \"./data/klej_cbd/train.tsv\"\n",
    "\n",
    "print(\"Reading vocab.\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.read_vocab('./data/counted_vocab.txt')\n",
    "tokenizer.change_to_reversible()\n",
    "print(\"Vocab of size:\", tokenizer.vocab_size, \"loaded.\")\n",
    "\n",
    "data_generator = TuningDataGenerator(text_file, max_len, tokenizer, batch_size=64, tuning_type=learn_type)\n",
    "print(\"Data generator prepared.\")\n",
    "\n",
    "sequence_encoder = create_model(tokenizer.vocab_size, max_len, embedding_dim, heads, encoder_num, ff_dim)\n",
    "print(\"Model created.\")\n",
    "\n",
    "# Start training.\n",
    "train = fine_tune(sequence_encoder, tokenizer, max_len, data_generator, epochs=epochs,\n",
    "                    checkpoint_file_path=new_checkpoint, load_checkpoint=True, old_checkpoint=old_checkpoint, learn_type=learn_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-Tune process -- CDSC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras_bert.tuning.tuning_data_generator import TuningDataGenerator\n",
    "from keras_bert.model import create_model\n",
    "from keras_bert.tokenizer import Tokenizer\n",
    "from keras_bert.tuning.fine_tuning import fine_tune\n",
    "\n",
    "max_len = 32\n",
    "embedding_dim = 512\n",
    "ff_dim = 512\n",
    "heads = 4\n",
    "encoder_num = 4\n",
    "\n",
    "epochs = 10\n",
    "learn_type = \"cdsc\"\n",
    "old_checkpoint = \"./data/checkpoint_test10.ckpt\"\n",
    "new_checkpoint = \"./data/klej_cdsc-e/checkpoint_cdsc.ckpt\"\n",
    "text_file = \"./data/klej_cdsc-e/train.tsv\"\n",
    "\n",
    "print(\"Reading vocab.\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.read_vocab('./data/counted_vocab.txt')\n",
    "tokenizer.change_to_reversible()\n",
    "print(\"Vocab of size:\", tokenizer.vocab_size, \"loaded.\")\n",
    "\n",
    "data_generator = TuningDataGenerator(text_file, max_len, tokenizer, batch_size=64, tuning_type=learn_type)\n",
    "print(\"Data generator prepared.\")\n",
    "\n",
    "sequence_encoder = create_model(tokenizer.vocab_size, max_len, embedding_dim, heads, encoder_num, ff_dim)\n",
    "print(\"Model created.\")\n",
    "\n",
    "# Start training.\n",
    "train = fine_tune(sequence_encoder, tokenizer, max_len, data_generator, epochs=epochs,\n",
    "                    checkpoint_file_path=new_checkpoint, load_checkpoint=True, old_checkpoint=old_checkpoint, learn_type=learn_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-Tune process -- DYK"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from keras_bert.tuning.tuning_data_generator import TuningDataGenerator\n",
    "from keras_bert.model import create_model\n",
    "from keras_bert.tokenizer import Tokenizer\n",
    "from keras_bert.tuning.fine_tuning import fine_tune\n",
    "\n",
    "max_len = 32\n",
    "embedding_dim = 512\n",
    "ff_dim = 512\n",
    "heads = 4\n",
    "encoder_num = 4\n",
    "\n",
    "epochs = 10\n",
    "learn_type = \"dyk\"\n",
    "old_checkpoint = \"./data/checkpoint_test10.ckpt\"\n",
    "new_checkpoint = \"./data/klej_dyk/checkpoint_dyk.ckpt\"\n",
    "text_file = \"./data/klej_dyk/train.tsv\"\n",
    "\n",
    "print(\"Reading vocab.\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.read_vocab('./data/counted_vocab.txt')\n",
    "tokenizer.change_to_reversible()\n",
    "print(\"Vocab of size:\", tokenizer.vocab_size, \"loaded.\")\n",
    "\n",
    "data_generator = TuningDataGenerator(text_file, max_len, tokenizer, batch_size=64, tuning_type=learn_type)\n",
    "print(\"Data generator prepared.\")\n",
    "\n",
    "sequence_encoder = create_model(tokenizer.vocab_size, max_len, embedding_dim, heads, encoder_num, ff_dim)\n",
    "print(\"Model created.\")\n",
    "\n",
    "# Start training.\n",
    "train = fine_tune(sequence_encoder, tokenizer, max_len, data_generator, epochs=epochs,\n",
    "                    checkpoint_file_path=new_checkpoint, load_checkpoint=True, old_checkpoint=old_checkpoint, learn_type=learn_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras_bert.tuning.tuning_data_generator import TuningDataGenerator\n",
    "from keras_bert.model import create_model\n",
    "from keras_bert.tokenizer import Tokenizer\n",
    "from keras_bert.tuning.fine_tuning import load_pretrained_model\n",
    "\n",
    "max_len = 32\n",
    "embedding_dim = 512\n",
    "ff_dim = 512\n",
    "heads = 4\n",
    "encoder_num = 4\n",
    "\n",
    "learn_type = \"dyk\"\n",
    "old_checkpoint = \"./data/klej_dyk/checkpoint_dyk.ckpt\"\n",
    "text_file = \"./data/klej_dyk/dev.tsv\"\n",
    "\n",
    "print(\"Reading vocab.\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.read_vocab('./data/counted_vocab.txt')\n",
    "tokenizer.change_to_reversible()\n",
    "print(\"Vocab of size:\", tokenizer.vocab_size, \"loaded.\")\n",
    "\n",
    "data_generator = TuningDataGenerator(text_file, max_len, tokenizer, batch_size=64, tuning_type=learn_type)\n",
    "print(\"Data generator prepared.\")\n",
    "\n",
    "sequence_encoder = create_model(tokenizer.vocab_size, max_len, embedding_dim, heads, encoder_num, ff_dim)\n",
    "print(\"Model created.\")\n",
    "\n",
    "model = load_pretrained_model(sequence_encoder,old_checkpoint,learn_type)\n",
    "\n",
    "# Evaluate.\n",
    "model.evaluate(x=data_generator)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ]
}